{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#data=pd.read_csv('train.csv', sep='\\t', encoding='utf8', error_bad_lines=False)\n",
    "#error_bad_lines=False\n",
    "#unsup = pd.read_csv('other.csv', sep='\\t', encoding='utf8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "число процессоров = 8\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "print( 'число процессоров = {}'.format( cpu_count() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Очистим данные от HTML\n",
    "#for i in data1 ['description']:\n",
    "#    data1['description'] = data1['description'].replace(i, BeautifulSoup(i, 'html5lib').get_text())\n",
    "#data1.info()\n",
    "#data1.to_csv('data1.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы не ждать каждый раз загружаем датафрейс уже без HTML тегов\n",
    "\n",
    "data1=pd.read_csv('data1.csv', sep='\\t', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = data1['description']\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test, y_train, y_test = \\\n",
    "train_test_split(data1, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Функция которая приводит к нижнему регистру и делит текст на слова\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def text_to_wordlist(text): \n",
    "    text = re.sub('[^а-яА-ЯёЁ]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция которая для представленного текста:\n",
    "\n",
    "удаляет html теги\n",
    "производит деления на предложения\n",
    "каждое предложение делит на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk.data\n",
    "def text_to_sentences(text):\n",
    "    #text = BeautifulSoup(text).get_text()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, ' ', text)\n",
    "    tokenizer = nltk.data.load('russian.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text_to_wordlist(raw_sentence))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все вместе:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for descr in train['description']:\n",
    "    sentences += text_to_sentences(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for descr in unsup['description']:\n",
    "#    sentences += text_to_sentences(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for descr in test['description']:\n",
    "    sentences += text_to_sentences(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# В конце сбора sentences будет list list'ов (список списков) - как и пример выше.\n",
    "# (Повторюсь) каждый элемент списка sentences - предложение, но представленное в виде списка слов - потому список\n",
    "\n",
    "# выведем количество элементов этого массива (оно же - количество предложений во всех текста)\n",
    "#print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Обучим же теперь модель Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ildar\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# симортируем соответствующую функцию из модуля gensim, который должен быть установлен\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# список параметров, которые можно менять по вашему желанию\n",
    "num_features = 150  # итоговая размерность вектора каждого слова\n",
    "min_word_count = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "num_workers = 8     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "context = 10        # размер окна \n",
    "downsampling = 1e-5 # внутренняя метрика модели\n",
    "\n",
    "model = Word2Vec(sentences, workers=num_workers, size=num_features,\n",
    "                 min_count=min_word_count, window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Финализируем нашу модель. Ее нельзя будет доучить теперь, но она начнет занимать гораздо меньше места\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "#Натренировав модель, получили представление каждого слова в семантическом пространстве (часто называют \"псевдо\" семантическое пространство)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. мы хотим классифицировать не слова, а тексты, надо перевести тексты в вектора (представить в виде фич) Один из простых методов - сложить все вектора слов входящих в текст и поделить на число входящих слов. Напишем функцию, которая:\n",
    "\n",
    "создает нулевой вектор - это будет результирующий вектор\n",
    "идем по всем словам в тексте, если слово есть в моделе:\n",
    "увеличиваем счетчик слов\n",
    "прибавим вектор слова к результирующему вектору\n",
    "поделим все координаты на число слов по которым, вектора которых мы прибавляли к результирующему вектору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "def text_to_vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            text_vec = np.add(text_vec, model[word])\n",
    "    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая на входе получает список всех текстов, а на выходе отдает список вектор каждого текста - что является прямоугольной матрицей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def texts_to_vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype=\"float32\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        texts_vecs[i] = text_to_vec(text, model, size)\n",
    "\n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим что функция texts_to_vecs принимает не просто тексты, а список всех слов текста.\n",
    "\n",
    "(!!!) Внимание: не список списков (там где сначала делили на предложения, а предложения на слова), а обычный линейный список\n",
    "\n",
    "Но у нас есть функции, которые переводят 1) текст в список предложений, 2) предложение в список слов\n",
    "\n",
    "Может показаться, что можно использовать 2ую функцию, но придется тогда ее переписать, потому как теги у нас удаляются лишь в первой функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Wall time: 823 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Поступим иначе, в python есть возможность развернуть двухмерный массив в одномерный, вот пример\n",
    "temp_list = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "print (sum(temp_list, []))\n",
    "\n",
    "# магия :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# действительно работает, сделаем для всех текстов из train\n",
    "train_like_word_list = [sum(text_to_sentences(text), []) for text in train['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ildar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs = texts_to_vecs(train_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# сделаем тоже самое для test\n",
    "test_like_word_list = [sum(text_to_sentences(text), []) for text in test['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_vecs = texts_to_vecs(test_like_word_list,model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспользуемся train_vecs, test_vecs, train['target'] \n",
    "#    как матрица фичей обучающей выборки, матрица фичей тестовой выборки, таргет для обучающей выборки соответственно\n",
    "\n",
    "# Стандартный случайный лес в таком случае\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs, train['target'])\n",
    "predict = forest.predict(test_vecs)\n",
    "\n",
    "# И вот задача решена\n",
    "\n",
    "##########################################################\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если для получения результирующего вектора не складываеть вектора, а пойти другим способом.\n",
    "\n",
    "Кластеризуем все слова на 1000 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В model.syn0 хранятся все вектора. Кластеризируем их!\n",
    "\n",
    "print ('Размер ', model.wv.syn0.shape)\n",
    "print ('Вектор векторов ', model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ildar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\Ildar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Кластеризируем все слова. \n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "# Число кластеров установим в 10000. Для этого числа нет \"серебряной пули\". Для каждого случая лучше подойдет разная\n",
    "num_clusters = 1000\n",
    "\n",
    "# Начнем кластеризацию, учитывая что классов много, количество векторов (по сути слов) много,\n",
    "#   все это будет происходит продолжительное время. Можно сходить за чаем.\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# в idx будут храниться номера классов для каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# создадим структуру dict (словарь): слово -> класс\n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "['найдет', 'выбирает', 'свете', 'сделав', 'вкусную', 'заставляют', 'живи', 'домашними', 'душа', 'демонстрирует', 'бриллиантов', 'оригинальным', 'расставаться', 'приветливая', 'позволяя', 'индивидуальности', 'особый', 'подарка', 'комфортом', 'ребенку', 'вещь', 'вернуть', 'мозга', 'способна', 'предлагаемый', 'маленькими', 'биться', 'демократичной', 'наполненными', 'окружающий', 'легкого', 'прожить', 'однажды', 'остался', 'здоровому', 'подчеркнуть', 'прибыльным', 'удовлетворяя', 'воплощает', 'меняя', 'радушно', 'слышит', 'мужчины', 'искренней', 'оставаясь', 'сориентироваться', 'слушает', 'незаменимым', 'покупками', 'удобству', 'убрать', 'творцом', 'близкими', 'долгую', 'ощущения', 'вдохновляющий', 'подготовленный', 'создай', 'девушкам', 'лояльно', 'требовательного', 'торговать', 'восторг', 'освещать', 'насладиться', 'женщине', 'посоветовать', 'эстетичным', 'наполненный', 'найдутся', 'подарить', 'самими', 'шедевр', 'придать', 'неизменным', 'превратиться', 'здоровом', 'эстетику', 'частичку', 'женщинам', 'веселье', 'раскрывая', 'зарядиться', 'доставляя', 'превосходный', 'понравившуюся', 'найдётся', 'миллена', 'посетителю', 'возвращались', 'равнодушными', 'наполняем', 'раскроем', 'ушел', 'рождает', 'наградой', 'пятой', 'предложил', 'искушенный', 'естественное', 'выражением', 'увлекается', 'вовлеки', 'вдохнови', 'исключительной', 'коже', 'спешит', 'пожилым', 'незабываемой', 'гуме', 'уважающим', 'преимуществе', 'получился', 'трогательный', 'страшит', 'нелегко', 'элегантным', 'задал', 'видном', 'располагают', 'связанному']\n",
      "Wall time: 31.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# выведем представителей первых 10 классов и посмотрим на адекватность произошедшей кластеризации\n",
    "xrange=range\n",
    "for cluster in xrange(0,10):\n",
    "    print (cluster)\n",
    "    #words = []\n",
    "    #for i in xrange(0, len(word_centroid_map.values())):\n",
    "    #    if word_centroid_map.values()[i] == cluster:\n",
    "    #        words.append(word_centroid_map.keys()[i])\n",
    "    #print (words)\n",
    "    \n",
    "    \n",
    "    words = []\n",
    "for key, item in word_centroid_map.items():\n",
    "    if item == cluster:\n",
    "        words.append(key)\n",
    "print (words)\n",
    "\n",
    "\n",
    "#или\n",
    "#words = [k for k, w in word_centroid_map.items() if w == cluster]\n",
    "    \n",
    "    \n",
    "    #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Существует несколько подходов к работе с кластерами слов. Рассмотрим 2 примера\n",
    "\n",
    "1) посчитаем для каждого текста вектор сколько его слов встретилось в каждом кластере. Т.е. для каждого текста будет вектор размера равного числу кластеров, значениями вектора будут натуральные числа\n",
    "2) Посчитаем усредненных удаленность векторов текстов от каждого центройда всех кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для первого случая напишем функцию, на вход которой поступает текст, представленный в виде списка всех его слов\n",
    "#    смотрим в каком кластере находится каждое слово \n",
    "#    и увеличиваем соответствующую ячейку (ответственную за этот кластер) на 1\n",
    "\n",
    "\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map, num_centroids):\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    set_word_centroid_map = set(word_centroid_map.keys())\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in set_word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Но нам нужно это не для одного текста, а для всех текстов обучающей и тестовой выборки\n",
    "# Сделаем это\n",
    "\n",
    "train_vecs_centroids = np.zeros((train['description'].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, text in enumerate(train_like_word_list):\n",
    "    train_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "\n",
    "#test_vecs_centroids = np.zeros((test['description'].size, num_clusters), dtype=\"float32\")\n",
    "#\n",
    "#for i, text in enumerate(test_like_word_list):\n",
    "#    test_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "#    \n",
    "# Результатом будет матрицы для обучающих и тестовых текстов, что будет матрицей фич обучающей и тестовой выборок соотв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids, train['target'])\n",
    "predict = forest.predict(test_vecs_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а теперь попробуем посчитать растояние от центройдов\n",
    "\n",
    "train_vecs_centroids_dist = np.zeros((train['description'].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "for i, vec in enumerate(train_vecs):\n",
    "    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "        train_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Всё что не нужно для итогового предсказания - закомментил. Чтобы не увеличивать время работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vecs_centroids_dist = np.zeros((test['description'].size, num_clusters), dtype=\"float32\")\n",
    "#\n",
    "#for i, vec in enumerate(test_vecs):\n",
    "#    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "#        test_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# стандартный случайный лес на полученных матрицах (кто бы сомневался)\n",
    "\n",
    "#forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "#forest = forest.fit(train_vecs_centroids_dist, train['target'])\n",
    "#predict = forest.predict(test_vecs_centroids_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД ЛОГИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "#from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "#logreg = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "#          intercept_scaling=50, max_iter=10, multi_class='ovr', n_jobs=1,\n",
    "#          penalty='l2', random_state=None, solver='liblinear', tol=1e-6,\n",
    "#          verbose=0, warm_start=False)\n",
    "##обучение логической регрессии\n",
    "#logreg = logreg.fit(train_vecs_centroids, train['target'])\n",
    "#predict = logreg.predict(test_vecs_centroids)\n",
    "#predict2 = logreg.predict(test_vecs_centroids_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После того как мы обучили модель сделаем предсказания для тестовой выборки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datatest=pd.read_csv('test.csv', sep='\\t', encoding='utf8')\n",
    "\n",
    "#for i in datatest ['description']:\n",
    "#    datatest['description'] = datatest['description'].replace(i, BeautifulSoup(i, 'html5lib').get_text())\n",
    "#datatest.info()\n",
    "#datatest.to_csv('datatest1.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "# Чтобы не считать каждый роз - загружаю уже готовую выборку без HTML\n",
    "datatest=pd.read_csv('datatest1.csv', sep='\\t', encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for descr in datatest['description']:\n",
    "    text_to_sentences(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# действительно работает, сделаем для всех текстов из train\n",
    "test_like_word_list = [sum(text_to_sentences(text), []) for text in datatest['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ildar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_vecs = texts_to_vecs(test_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.spatial import distance\n",
    "test_vecs_centroids = np.zeros((datatest['description'].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, text in enumerate(test_like_word_list):\n",
    "    test_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "    \n",
    "# Результатом будет матрица для  текста, что будет матрицей фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs_centroids_dist = np.zeros((datatest['description'].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, vec in enumerate(test_vecs):\n",
    "    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "        test_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170179, 1000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандарт случ лес\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs, train['target'])\n",
    "predict = forest.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids, train['target'])\n",
    "predict1 = forest.predict(test_vecs_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах (кто бы сомневался)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids_dist, train['target'])\n",
    "predict2 = forest.predict(test_vecs_centroids_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запишем все по файлам для загрузки на Kaggle\n",
    "df_id = datatest['id']\n",
    "\n",
    "df_predict = pd.DataFrame(predict)\n",
    "df_pr=df_predict[0]\n",
    "# Объединяем таргет и ID в датафрейм\n",
    "df_predict1=pd.concat([df_id, df_pr], axis=1)\n",
    "df_predict1.columns = ['id', 'target']\n",
    "df_predict1.to_csv('predict1.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "df_predict = pd.DataFrame(predict1)\n",
    "df_pr=df_predict[0]\n",
    "# Объединяем таргет и ID в датафрейм\n",
    "df_predict2=pd.concat([df_id, df_pr], axis=1)\n",
    "df_predict2.columns = ['id', 'target']\n",
    "df_predict2.to_csv('predict2.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "df_predict = pd.DataFrame(predict2)\n",
    "df_pr=df_predict[0]\n",
    "# Объединяем таргет и ID в датафрейм\n",
    "df_predict3=pd.concat([df_id, df_pr], axis=1)\n",
    "df_predict3.columns = ['id', 'target']\n",
    "df_predict3.to_csv('predict3.csv', sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД ЛОГИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "logreg = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=50, max_iter=10, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=1e-6,\n",
    "          verbose=0, warm_start=False)\n",
    "#обучение логической регрессии\n",
    "logreg = logreg.fit(train_vecs_centroids, train['target'])\n",
    "predict2_1 = logreg.predict(test_vecs_centroids)\n",
    "predict2_2 = logreg.predict(test_vecs_centroids_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame(predict2_1)\n",
    "df_pr=df_predict[0]\n",
    "# Объединяем таргет и ID в датафрейм\n",
    "df_predict=pd.concat([df_id, df_pr], axis=1)\n",
    "df_predict.columns = ['id', 'target']\n",
    "df_predict.to_csv('predict2_1.csv', sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "df_predict = pd.DataFrame(predict2_2)\n",
    "df_pr=df_predict[0]\n",
    "# Объединяем таргет и ID в датафрейм\n",
    "df_predict=pd.concat([df_id, df_pr], axis=1)\n",
    "df_predict.columns = ['id', 'target']\n",
    "df_predict.to_csv('predict2_2.csv', sep=',', encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
